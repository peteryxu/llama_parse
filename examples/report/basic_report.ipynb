{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Generation with LlamaReport\n",
    "\n",
    "In this notebook, we'll walk through the basic process of generating a report with LlamaReport, and highlight some of the key features of the library.\n",
    "\n",
    "TLDR:\n",
    "1. Download source data to use as knowledge base for the report\n",
    "2. Kick off report generation with a template\n",
    "3. Get the plan and review/accept/reject suggestions\n",
    "4. Get the final report\n",
    "5. Review/accept/reject suggestions to edit the final report\n",
    "6. Print the final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-cloud-services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Source Data\n",
    "\n",
    "Here, we download the `Attention is All You Need` paper as a PDF.\n",
    "\n",
    "LlamaReport currently supports up to 5 files as input, and essentially any file type that can be parsed by LlamaParse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://arxiv.org/pdf/1706.03762.pdf\" -O \"./attention.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kick off Report Generation\n",
    "\n",
    "Here, we kick off report generation with a template.\n",
    "\n",
    "The template can either be a string or a file path, but here we'll use a string.\n",
    "\n",
    "In our experiments, anything works as a template, but some general guidelines:\n",
    "\n",
    "- Use markdown formatting + instructions in each section to guide the report generation\n",
    "- If using an existing file as a template, provide extra instructions to guide the report generation\n",
    "\n",
    "**NOTE:** Since we are in a notebook, we will use async functions and `await` throughout. Synchronous methods that work without `await` are available by just removing the `a` from the method name and removing the `await` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cloud_services import LlamaReport\n",
    "\n",
    "llama_report = LlamaReport(\n",
    "    api_key=\"llx-...\",\n",
    ")\n",
    "\n",
    "report_client = await llama_report.acreate_report(\n",
    "    name=\"my_cool_report_on_attention\",\n",
    "    # can pass in file paths or bytes\n",
    "    input_files=[\"./attention.pdf\"],\n",
    "    template_text=\"\"\"\\\n",
    "# [Some title]\\n\\n\n",
    "## TLDR\\n\n",
    "A quick summary of the paper.\\n\\n\n",
    "## Details\\n\n",
    "More details about the paper, possibly more than one section here.\\n\n",
    "\"\"\",\n",
    "    # optional additional instructions for the report generation\n",
    "    # template_instructions=None,\n",
    "    # optional file path to an existing template instead of template_text\n",
    "    # template_file=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned `ReportClient` object is used to interact with the report generation process for this specific report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report(id=0a394b33-1a3e-463c-b5cb-7ff8ab827d0a, name=my_cool_report_on_attention)\n"
     ]
    }
   ],
   "source": [
    "print(report_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get the plan\n",
    "\n",
    "The first phases of report generation involve ingesting the source data and generating a plan.\n",
    "\n",
    "The plan is a list of instructions for the report generation, and can be reviewed/accepted/rejected by the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = await report_client.await_for_plan(\n",
    "    timeout=10000,\n",
    "    poll_interval=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# {title}\n",
      "[ReportQuery(field='title', prompt='Generate a clear and concise title for this paper about the Transformer model and attention mechanisms', context='The paper discusses the Transformer architecture for sequence transduction using attention mechanisms, focusing on machine translation applications')]\n",
      "==================\n",
      "## TLDR\n",
      "\n",
      "{tldr_content}\n",
      "[ReportQuery(field='tldr_content', prompt='Write a brief, clear summary of the key points about the Transformer model', context='Focus on the main innovations: attention mechanisms, efficiency improvements, and state-of-the-art results in machine translation')]\n",
      "==================\n",
      "## Details\n",
      "\n",
      "{details_content}\n",
      "[ReportQuery(field='details_content', prompt='Provide detailed information about the Transformer model architecture and its applications', context='Include information about:\\n- The attention mechanism implementation\\n- Advantages over recurrent and convolutional models\\n- Performance in machine translation tasks\\n- Training efficiency improvements')]\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "for plan_block in plan.blocks:\n",
    "    print(plan_block.block.template)\n",
    "    print(plan_block.queries)\n",
    "    print(\"==================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the plan, we can either use it to kick off generation of the final report, or we can edit the plan and adjust it as needed.\n",
    "\n",
    "While we could manually edit the objects here and use `await report_client.aupdate_plan(action=\"edit\", updated_plan=plan)`, we can also use `LlamaReport` to agentically edit the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestions = await report_client.asuggest_edits(\n",
    "    \"Can you split the details section into two sections?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Justification for change: \n",
      "I'll help you break down the details section into two distinct parts - one focusing on the architecture and another on the practical applications and performance. This will make the content more organized and easier to follow. The original block at index 2 will be replaced with these two new sections.\n",
      "\n",
      "Proposed changes:\n",
      "\n",
      "## Architecture Details\n",
      "\n",
      "{architecture_content}\n",
      "\n",
      "[ReportQuery(field='architecture_content', prompt='Describe the technical details of the Transformer model architecture', context='Focus on:\\n- Core components of the Transformer architecture\\n- Self-attention mechanism implementation\\n- Multi-head attention details\\n- Position encoding approach\\n- Feed-forward network structure')]\n",
      "==================\n",
      "\n",
      "## Performance and Applications\n",
      "\n",
      "{applications_content}\n",
      "\n",
      "[ReportQuery(field='applications_content', prompt='Explain the practical applications and performance advantages of the Transformer model', context='Cover:\\n- Comparison with RNN and CNN models\\n- Machine translation results and benchmarks\\n- Training efficiency improvements\\n- Real-world applications and use cases\\n- Scalability benefits')]\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "for suggestion in suggestions:\n",
    "    print(\"Justification for change:\", suggestion.justification)\n",
    "    print(\"Proposed changes:\")\n",
    "    for plan_block in suggestion.blocks:\n",
    "        print(plan_block.block.template)\n",
    "        print(plan_block.queries)\n",
    "        print(\"==================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good! We can also use the client to automatically accept and apply, or reject, these suggestions.\n",
    "\n",
    "This will (locally) keep track of the history of changes, so that future suggestions can be based on the previous changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for suggestion in suggestions:\n",
    "    await report_client.aaccept_edit(suggestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What effect did that have on the tracked local history? Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EditAction(block_idx=2, old_content='## Details\\n\\n{details_content}\\n\\nField: details_content, Prompt: Provide detailed information about the Transformer model architecture and its applications, Context: Include information about:\\n- The attention mechanism implementation\\n- Advantages over recurrent and convolutional models\\n- Performance in machine translation tasks\\n- Training efficiency improvements\\nDepends on: none', new_content='\\n## Architecture Details\\n\\n{architecture_content}\\n\\n\\nField: architecture_content, Prompt: Describe the technical details of the Transformer model architecture, Context: Focus on:\\n- Core components of the Transformer architecture\\n- Self-attention mechanism implementation\\n- Multi-head attention details\\n- Position encoding approach\\n- Feed-forward network structure\\nDepends on: none', action='approved', timestamp=datetime.datetime(2025, 2, 4, 20, 59, 55, 773558)),\n",
       " EditAction(block_idx=3, old_content='[No old content]', new_content='\\n## Performance and Applications\\n\\n{applications_content}\\n\\n\\nField: applications_content, Prompt: Explain the practical applications and performance advantages of the Transformer model, Context: Cover:\\n- Comparison with RNN and CNN models\\n- Machine translation results and benchmarks\\n- Training efficiency improvements\\n- Real-world applications and use cases\\n- Scalability benefits\\nDepends on: previous', action='approved', timestamp=datetime.datetime(2025, 2, 4, 20, 59, 55, 773687))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_client.edit_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Message(role=<MessageRole.USER: 'user'>, content='Can you split the details section into two sections?', timestamp=datetime.datetime(2025, 2, 4, 20, 59, 47, 754848)),\n",
       " Message(role=<MessageRole.ASSISTANT: 'assistant'>, content=\"\\nI'll help you break down the details section into two distinct parts - one focusing on the architecture and another on the practical applications and performance. This will make the content more organized and easier to follow. The original block at index 2 will be replaced with these two new sections.\\n\", timestamp=datetime.datetime(2025, 2, 4, 20, 59, 55, 482070))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_client.chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two items are used to provide context for future suggestions! You can always clear this, or provide your own history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report_client.suggest_edits(\"....\", chat_history=[{\"role\": \"user\", \"content\": \"...\"}, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get the final report\n",
    "\n",
    "Now that we have a plan, we can kick off generation of the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kicks off report generation\n",
    "await report_client.aupdate_plan(action=\"approve\")\n",
    "\n",
    "# waits for report generation to complete\n",
    "report = await report_client.await_completion(\n",
    "    timeout=10000,\n",
    "    poll_interval=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Attention Is All You Need: A Pure Attention-Based Architecture for Neural Machine Translation\n",
      "\n",
      "## TLDR\n",
      "\n",
      "The Transformer introduced a revolutionary architecture that relies entirely on attention mechanisms, eliminating the need for recurrence or convolution in sequence processing. Its key innovations include multi-head self-attention for parallel processing of input sequences, scaled dot-product attention for efficient computation, and positional encodings for sequence order awareness. The model achieved breakthrough results in machine translation (28.4 BLEU on English-to-German, 41.8 BLEU on English-to-French) while requiring significantly less training time than previous approaches, training in 3.5 days on 8 GPUs. This architecture demonstrated that attention mechanisms alone are sufficient for state-of-the-art sequence modeling, setting a new direction for natural language processing.\n",
      "\n",
      "\n",
      "## Architecture Details\n",
      "\n",
      "The Transformer architecture represents a groundbreaking approach to sequence processing, built entirely on attention mechanisms without recurrence or convolution. Here are its key technical details:\n",
      "\n",
      "Core Components:\n",
      "- Encoder-decoder architecture with stacked self-attention and point-wise feed-forward layers\n",
      "- Each layer contains two main sub-layers: multi-head self-attention mechanism and position-wise feed-forward network\n",
      "- Layer normalization and residual connections between sub-layers\n",
      "- No recurrent or convolutional elements, enabling parallel processing\n",
      "\n",
      "Self-Attention Mechanism:\n",
      "- Processes relationships between all positions in a sequence simultaneously\n",
      "- Computes attention weights using queries, keys, and values derived from input representations\n",
      "- Implements scaled dot-product attention to prevent gradient issues with large input dimensions\n",
      "- Allows direct modeling of dependencies regardless of positional distance\n",
      "- Uses masking in decoder to prevent leftward information flow and maintain auto-regressive property\n",
      "\n",
      "Multi-Head Attention:\n",
      "- Employs multiple attention heads operating in parallel\n",
      "- Each head processes information in different representation subspaces\n",
      "- Three types of attention applications:\n",
      "  1. Encoder self-attention (all positions attend to each other)\n",
      "  2. Decoder self-attention (each position attends to previous positions)\n",
      "  3. Encoder-decoder attention (decoder queries attend to encoder outputs)\n",
      "- Counteracts reduced resolution from attention averaging through parallel processing\n",
      "\n",
      "Position-wise Feed-Forward Network:\n",
      "- Applied identically to each position separately\n",
      "- Consists of two linear transformations with ReLU activation\n",
      "- Structure: FFN(x) = max(0, xW1 + b1)W2 + b2\n",
      "- Input and output dimensionality: dmodel = 512\n",
      "- Inner-layer dimensionality: dff = 2048\n",
      "- Parameters vary between layers but remain constant across positions\n",
      "\n",
      "Position Encoding:\n",
      "- Adds positional information to input embeddings\n",
      "- Enables the model to consider sequential order without recurrence\n",
      "- Implements sinusoidal position encodings to allow model to attend to relative positions\n",
      "- Maintains constant number of operations between any two positions, unlike convolutional approaches\n",
      "- Allows effective modeling of both local and long-range dependencies\n",
      "\n",
      "\n",
      "\n",
      "## Performance and Applications\n",
      "\n",
      "The Transformer model demonstrates significant performance advantages and practical applications across multiple domains:\n",
      "\n",
      "Performance Advantages over RNN/CNN Models:\n",
      "- Eliminates sequential computation constraints present in RNNs, enabling superior parallelization\n",
      "- Reduces operations needed for relating distant positions to a constant number, compared to linear/logarithmic scaling in CNNs\n",
      "- Processes all input and output positions simultaneously through self-attention mechanisms\n",
      "- Achieves state-of-the-art results while requiring significantly less computational resources\n",
      "\n",
      "Machine Translation Benchmarks:\n",
      "- WMT 2014 English-to-German: 28.4 BLEU score, exceeding previous best results by over 2 BLEU points\n",
      "- WMT 2014 English-to-French: 41.8 BLEU score (single-model state-of-the-art)\n",
      "- Surpasses performance of existing model ensembles in translation tasks\n",
      "\n",
      "Training Efficiency:\n",
      "- Requires only 3.5 days of training on eight GPUs for state-of-the-art performance\n",
      "- Achieves superior results at \"a small fraction of the training costs\" compared to previous models\n",
      "- Enables significantly faster training through parallel processing of input/output sequences\n",
      "- Can reach production-quality performance in as little as twelve hours on modern GPU hardware\n",
      "\n",
      "Real-world Applications:\n",
      "- Machine translation systems\n",
      "- Natural language understanding tasks\n",
      "- Reading comprehension\n",
      "- Abstractive summarization\n",
      "- Text entailment analysis\n",
      "- Constituency parsing (achieving 92.7 F1 score in semi-supervised settings)\n",
      "- Adaptable to both large and limited training data scenarios\n",
      "\n",
      "Scalability Benefits:\n",
      "- Highly parallelizable architecture enables efficient scaling across multiple GPUs\n",
      "- Constant computational complexity for relating any input/output positions\n",
      "- Effective handling of long-range dependencies in sequences\n",
      "- Maintains performance quality while scaling to larger datasets and model sizes\n",
      "- Generalizes well across different tasks and domains without architectural changes\n",
      "- Supports efficient inference and deployment in production environments\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_text = \"\\n\\n\".join([block.template for block in report.blocks])\n",
    "print(report_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Edit the final report\n",
    "\n",
    "Now that we have a report, we can edit it.\n",
    "\n",
    "We can use the `asuggest_edits` method to get suggestions for edits, and then use the `aaccept_edit`/`areject_edit` methods to apply them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Justification for change: \n",
      "I'd suggest changing \"TLDR\" to \"Executive Summary\" which is more appropriate for a professional or academic report. This term is widely used in formal documents and better reflects the nature of this concise overview section while maintaining the same function of providing a quick summary of the key points.\n",
      "\n",
      "Proposed changes:\n",
      "## Executive Summary\n",
      "\n",
      "The Transformer introduced a revolutionary architecture that relies entirely on attention mechanisms, eliminating the need for recurrence or convolution in sequence processing. Its key innovations include multi-head self-attention for parallel processing of input sequences, scaled dot-product attention for efficient computation, and positional encodings for sequence order awareness. The model achieved breakthrough results in machine translation (28.4 BLEU on English-to-German, 41.8 BLEU on English-to-French) while requiring significantly less training time than previous approaches, training in 3.5 days on 8 GPUs. This architecture demonstrated that attention mechanisms alone are sufficient for state-of-the-art sequence modeling, setting a new direction for natural language processing.\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "suggestions = await report_client.asuggest_edits(\n",
    "    \"Can you change the TLDR header to something more professional?\"\n",
    ")\n",
    "for suggestion in suggestions:\n",
    "    print(\"Justification for change:\", suggestion.justification)\n",
    "    print(\"Proposed changes:\")\n",
    "    for block in suggestion.blocks:\n",
    "        print(block.template)\n",
    "        print(\"==================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing to \"Executive Summary\" sounds reasonable, lets accept that!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for suggestion in suggestions:\n",
    "    await report_client.aaccept_edit(suggestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Print the final report\n",
    "\n",
    "Now that we have a report, we can print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Attention Is All You Need: A Pure Attention-Based Architecture for Neural Machine Translation\n",
      "\n",
      "## Executive Summary\n",
      "\n",
      "The Transformer introduced a revolutionary architecture that relies entirely on attention mechanisms, eliminating the need for recurrence or convolution in sequence processing. Its key innovations include multi-head self-attention for parallel processing of input sequences, scaled dot-product attention for efficient computation, and positional encodings for sequence order awareness. The model achieved breakthrough results in machine translation (28.4 BLEU on English-to-German, 41.8 BLEU on English-to-French) while requiring significantly less training time than previous approaches, training in 3.5 days on 8 GPUs. This architecture demonstrated that attention mechanisms alone are sufficient for state-of-the-art sequence modeling, setting a new direction for natural language processing.\n",
      "\n",
      "\n",
      "## Architecture Details\n",
      "\n",
      "The Transformer architecture represents a groundbreaking approach to sequence processing, built entirely on attention mechanisms without recurrence or convolution. Here are its key technical details:\n",
      "\n",
      "Core Components:\n",
      "- Encoder-decoder architecture with stacked self-attention and point-wise feed-forward layers\n",
      "- Each layer contains two main sub-layers: multi-head self-attention mechanism and position-wise feed-forward network\n",
      "- Layer normalization and residual connections between sub-layers\n",
      "- No recurrent or convolutional elements, enabling parallel processing\n",
      "\n",
      "Self-Attention Mechanism:\n",
      "- Processes relationships between all positions in a sequence simultaneously\n",
      "- Computes attention weights using queries, keys, and values derived from input representations\n",
      "- Implements scaled dot-product attention to prevent gradient issues with large input dimensions\n",
      "- Allows direct modeling of dependencies regardless of positional distance\n",
      "- Uses masking in decoder to prevent leftward information flow and maintain auto-regressive property\n",
      "\n",
      "Multi-Head Attention:\n",
      "- Employs multiple attention heads operating in parallel\n",
      "- Each head processes information in different representation subspaces\n",
      "- Three types of attention applications:\n",
      "  1. Encoder self-attention (all positions attend to each other)\n",
      "  2. Decoder self-attention (each position attends to previous positions)\n",
      "  3. Encoder-decoder attention (decoder queries attend to encoder outputs)\n",
      "- Counteracts reduced resolution from attention averaging through parallel processing\n",
      "\n",
      "Position-wise Feed-Forward Network:\n",
      "- Applied identically to each position separately\n",
      "- Consists of two linear transformations with ReLU activation\n",
      "- Structure: FFN(x) = max(0, xW1 + b1)W2 + b2\n",
      "- Input and output dimensionality: dmodel = 512\n",
      "- Inner-layer dimensionality: dff = 2048\n",
      "- Parameters vary between layers but remain constant across positions\n",
      "\n",
      "Position Encoding:\n",
      "- Adds positional information to input embeddings\n",
      "- Enables the model to consider sequential order without recurrence\n",
      "- Implements sinusoidal position encodings to allow model to attend to relative positions\n",
      "- Maintains constant number of operations between any two positions, unlike convolutional approaches\n",
      "- Allows effective modeling of both local and long-range dependencies\n",
      "\n",
      "\n",
      "\n",
      "## Performance and Applications\n",
      "\n",
      "The Transformer model demonstrates significant performance advantages and practical applications across multiple domains:\n",
      "\n",
      "Performance Advantages over RNN/CNN Models:\n",
      "- Eliminates sequential computation constraints present in RNNs, enabling superior parallelization\n",
      "- Reduces operations needed for relating distant positions to a constant number, compared to linear/logarithmic scaling in CNNs\n",
      "- Processes all input and output positions simultaneously through self-attention mechanisms\n",
      "- Achieves state-of-the-art results while requiring significantly less computational resources\n",
      "\n",
      "Machine Translation Benchmarks:\n",
      "- WMT 2014 English-to-German: 28.4 BLEU score, exceeding previous best results by over 2 BLEU points\n",
      "- WMT 2014 English-to-French: 41.8 BLEU score (single-model state-of-the-art)\n",
      "- Surpasses performance of existing model ensembles in translation tasks\n",
      "\n",
      "Training Efficiency:\n",
      "- Requires only 3.5 days of training on eight GPUs for state-of-the-art performance\n",
      "- Achieves superior results at \"a small fraction of the training costs\" compared to previous models\n",
      "- Enables significantly faster training through parallel processing of input/output sequences\n",
      "- Can reach production-quality performance in as little as twelve hours on modern GPU hardware\n",
      "\n",
      "Real-world Applications:\n",
      "- Machine translation systems\n",
      "- Natural language understanding tasks\n",
      "- Reading comprehension\n",
      "- Abstractive summarization\n",
      "- Text entailment analysis\n",
      "- Constituency parsing (achieving 92.7 F1 score in semi-supervised settings)\n",
      "- Adaptable to both large and limited training data scenarios\n",
      "\n",
      "Scalability Benefits:\n",
      "- Highly parallelizable architecture enables efficient scaling across multiple GPUs\n",
      "- Constant computational complexity for relating any input/output positions\n",
      "- Effective handling of long-range dependencies in sequences\n",
      "- Maintains performance quality while scaling to larger datasets and model sizes\n",
      "- Generalizes well across different tasks and domains without architectural changes\n",
      "- Supports efficient inference and deployment in production environments\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_response = await report_client.aget()\n",
    "report_text = \"\\n\\n\".join([block.template for block in report_response.report.blocks])\n",
    "print(report_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the sources for each block!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99687636\n",
      "# Abstract\n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutiona\n",
      "==================\n",
      "0.99591404\n",
      "# 2 Background\n",
      "\n",
      "The goal of reducing sequential computation also forms the foundation of the Extende\n",
      "==================\n",
      "0.9951325\n",
      "# 1 Introduction\n",
      "\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neu\n",
      "==================\n",
      "0.99442345\n",
      "# 7 Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduction model ba\n",
      "==================\n",
      "0.9967649\n",
      "# 3.2.3 Applications of Attention in our Model\n",
      "\n",
      "The Transformer uses multi-head attention in three d\n",
      "==================\n",
      "0.99533635\n",
      "# 2 Background\n",
      "\n",
      "The goal of reducing sequential computation also forms the foundation of the Extende\n",
      "==================\n",
      "0.9935868\n",
      "# Abstract\n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutiona\n",
      "==================\n",
      "0.98780584\n",
      "# Outputs\n",
      "\n",
      "(shifted right)\n",
      "\n",
      "Figure 1: The Transformer - model architecture.\n",
      "\n",
      "The Transformer follows\n",
      "==================\n",
      "0.9205043\n",
      "# 3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "In addition to attention sub-layers, each of the layers i\n",
      "==================\n",
      "0.79581684\n",
      "# 1 Introduction\n",
      "\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neu\n",
      "==================\n",
      "0.9946774\n",
      "# Abstract\n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutiona\n",
      "==================\n",
      "0.97079873\n",
      "# 7 Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduction model ba\n",
      "==================\n",
      "0.9535353\n",
      "# 6.3 English Constituency Parsing\n",
      "\n",
      "To evaluate if the Transformer can generalize to other tasks we \n",
      "==================\n",
      "0.9514138\n",
      "# 2 Background\n",
      "\n",
      "The goal of reducing sequential computation also forms the foundation of the Extende\n",
      "==================\n",
      "0.9790758\n",
      "# 1 Introduction\n",
      "\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neu\n",
      "==================\n",
      "0.92262185\n",
      "# Outputs\n",
      "\n",
      "(shifted right)\n",
      "\n",
      "Figure 1: The Transformer - model architecture.\n",
      "\n",
      "The Transformer follows\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "for block in report_response.report.blocks:\n",
    "    # Each block has a list of sources, which are the nodes that were used to generate the block\n",
    "    for source in block.sources:\n",
    "        print(source.score)\n",
    "        print(source.node.text[:100])\n",
    "        print(\"==================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-parse-aNC435Vv-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
